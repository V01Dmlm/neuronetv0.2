# ðŸ§  NeuroNet v0.1 â€” Offline AI Research Assistant  
*ChatGPT meets research. Local, fast, and private.*

---

## ðŸ“Œ What is NeuroNet?

**NeuroNet** is a self-hosted, privacy-first AI assistant for research and summarization. Designed to work **fully offline**, even on **low-spec machines**, it allows you to:

- ðŸ” Search the web (or load PDFs, `.txt`, etc.)
- âœï¸ Summarize content using **open-source LLMs** (GPT-Neo)
- ðŸ–¥ï¸ Interact via a clean PyQt6 **desktop GUI**
- ðŸ§  Run with **no API keys**, **no OpenAI**, and **no spying**
- ðŸª› Deploy on **budget-friendly hardware**

---

## âš™ï¸ Features

| Feature                        | Status |
|-------------------------------|--------|
| ðŸ”Œ Offline Mode               | âœ…     |
| ðŸ§  GPT-Neo Integration        | âœ…     |
| ðŸ” DuckDuckGo Web Search     | âœ…     |
| ðŸ“„ PDF + Text Reader         | âœ…     |
| ðŸ§° Vector Memory (FAISS/Chroma) | ðŸ”œ     |
| ðŸ–¥ï¸ PyQt6 GUI                  | âœ…     |
| ðŸ“± Android Packaging (Kivy)   | ðŸ”œ     |
| ðŸ’¬ CLI Support                | âœ…     |

---

## ðŸ§  How It Works

1. **User types a query**
2. **DuckDuckGo search** finds top articles (or loads local files)
3. Extracted text is passed to a **GPT-Neo summarizer**
4. **Concise summary** is generated
5. Displayed in either:
   - ðŸ–¥ï¸ PyQt6 GUI
   - ðŸ§‘â€ðŸ’» CLI output

---

## ðŸ–¥ï¸ Dev Notes

> âš ï¸ This is the **developer version**.  
> The user-friendly packaged version is under construction â€” current builds are too heavy for most machines without proper GPU or RAM.

---

## ðŸ§ª Target Hardware (Dev Tested)

| Component     | Spec                      |
|--------------|---------------------------|
| CPU          | Intel Core i7 (4th Gen)   |
| GPU          | AMD FirePro M4100 (1GB VRAM) |
| RAM          | 16GB DDR3 (2Ã—8GB)         |
| OS           | Arch Linux (LXQt)         |
| Python       | 3.11.5 (via pyenv)        |


## â“ Tech FAQ

### ðŸ”Œ Can this run completely offline?
**Yes.** No OpenAI, no cloud APIs, no data leaving your machine. You can summarize PDFs, `.txt`, and `.docx` files locally. Web search is optional.

---

### ðŸ§  Does it use GPT-4 or ChatGPT?
**Nope.** It uses **open-source models** like GPT-Neo. That means no API fees, no spying, no server limits â€” but also less raw power than OpenAI.

---

### ðŸ–¥ï¸ Will it work on my 2010 laptop?
If you're rocking:
- A 4th Gen Intel i7 or better  
- At least **8â€“16 GB of RAM**  
- A working Linux or Windows OS  

Then **yes** â€” even if your GPU is some crusty 1GB AMD card (*cough FirePro*).

---

### âš™ï¸ Does better hardware improve results?
**Absolutely.** Here's how:

| Better Hardware = | What You Get |
|-------------------|--------------|
| More RAM          | Longer input context (better summaries) |
| Faster CPU        | Lower inference time (less lag)         |
| Dedicated GPU     | Optional speed-up, especially for larger models |
| SSD/Cache space   | Faster model load and memory access     |

> GPT-Neo doesnâ€™t "learn" in real-time, but **your hardware controls how much it can process per query**.

---

### ðŸŒ Why is it slow on my machine?
Probably because:
- You're on CPU-only inference  
- You're loading a large model (e.g. GPT-Neo 1.3B+)  
- Youâ€™re running multiple tools (GUI, web scraper, vector store)  

Try the CLI mode for speed, or load smaller files.

---

### ðŸ“¦ Can I compile it into an `.exe` or `.AppImage`?
Yup. It supports **PyInstaller** for packaging


## ðŸš€ Future Improvements

Hereâ€™s whatâ€™s cooking for upcoming versions of NeuroNet:

### ðŸ”¬ Intelligence & Reasoning
- [ ] Add tool-based agent reasoning (LangChain-style)
- [ ] Question refinement + follow-up chaining
- [ ] Context blending: combine Web + PDF + Notes into one summary

### ðŸ§  Memory & Learning
- [ ] Vector memory support (FAISS or ChromaDB)
- [ ] User session memory for continuity
- [ ] Local topic tagging & recall system

### ðŸŒ Web & File Tools
- [ ] Auto-download articles from URLs
- [ ] Support for `.docx`, `.epub`, `.html`
- [ ] YouTube transcription + summarization
- [ ] Local directory indexing (like a personal research archive)

### ðŸŽ¨ GUI & UX
- [ ] Chat-style interface with markdown rendering
- [ ] File drag & drop
- [ ] Dark mode / Light mode toggle
- [ ] System tray & background mode

### ðŸ“± Mobile & Lightweight Builds
- [ ] Kivy-based Android version
- [ ] Voice input (offline STT)
- [ ] Model quantization for smaller footprints (GPT-J, GPT-2, etc.)

### âš™ï¸ Backend & Performance
- [ ] Model auto-downloader with resume support
- [ ] Modular agent architecture (plug in new tools easily)
- [ ] Async loading & task queuing (less blocking)
- [ ] GPU acceleration toggle

### ðŸ§ª Experimental Ideas
- [ ] Local graph-based citation mapping
- [ ] Research trend analyzer (arxiv / Semantic Scholar)
- [ ] â€œExplain like Iâ€™m 5â€ mode
- [ ] Plugin system (custom tools, local APIs)

---

> Want to help build one of these features? [Open an issue](https://github.com/your-repo/issues) or make a pull request!
